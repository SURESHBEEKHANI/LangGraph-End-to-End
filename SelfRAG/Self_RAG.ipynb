{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RAG Workflow ‚Äì StateGraph Summary**\n",
    "\n",
    "#### üîπ Overview\n",
    "This code defines a complete **Retrieval-Augmented Generation (RAG)** pipeline using **LangGraph StateGraph**.  \n",
    "Each node represents a processing step, and edges define the workflow logic.\n",
    "\n",
    "---\n",
    "\n",
    "###  Nodes\n",
    "\n",
    "- **retrieve** ‚Üí Fetch relevant documents  \n",
    "- **grade_documents** ‚Üí Evaluate document relevance  \n",
    "- **generate** ‚Üí Produce answer using retrieved context  \n",
    "- **transform_query** ‚Üí Rewrite query if documents are not relevant  \n",
    "\n",
    "---\n",
    "\n",
    "### Workflow Steps\n",
    "\n",
    "1. **START ‚Üí retrieve**  \n",
    "2. **retrieve ‚Üí grade_documents**  \n",
    "3. Conditional Flow:\n",
    "   - If documents are **relevant** ‚Üí `generate`  \n",
    "   - If **not relevant** ‚Üí `transform_query ‚Üí retrieve`  \n",
    "4. After generation:\n",
    "   - If answer is **useful** ‚Üí `END`  \n",
    "   - If **not supported** ‚Üí Regenerate  \n",
    "   - If **not useful** ‚Üí `transform_query ‚Üí retrieve`  \n",
    "\n",
    "---\n",
    "\n",
    "###  Key Insight\n",
    "This workflow creates a **self-correcting and adaptive RAG system** that:\n",
    "- Improves retrieval quality  \n",
    "- Validates answer grounding  \n",
    "- Iteratively refines queries  \n",
    "- Ensures accurate and context-aware responses  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load environment variables from.env**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'os' module to interact with the operating system environment variables\n",
    "import os\n",
    "\n",
    "# Import 'load_dotenv' from 'dotenv' to load environment variables from a .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file into the environment\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve API keys from environment variables and store them in Python variables\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")      # Google API key\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")      # Tavily API key\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")          # Groq API key\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")# LangChain API key\n",
    "\n",
    "# Set the API keys as environment variables for use in the application\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
    "\n",
    "# Additional LangChain configuration\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"                     # Enable LangChain tracing (for debugging/monitoring)\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"  # Set custom LangChain API endpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Required Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pprint module to \"pretty-print\" Python data structures for easier readability\n",
    "import pprint  \n",
    "\n",
    "# Import RecursiveCharacterTextSplitter from langchain_text_splitters.character\n",
    "# This is used to split large text documents into smaller chunks recursively,\n",
    "# based on characters, sentences, or paragraphs. Useful for processing text in LLM pipelines.\n",
    "from langchain_text_splitters.character import RecursiveCharacterTextSplitter  \n",
    "\n",
    "# Import WebBaseLoader from langchain_community.document_loaders\n",
    "# This loader allows fetching and loading documents directly from web pages (URLs) into your program.\n",
    "from langchain_community.document_loaders import WebBaseLoader  \n",
    "\n",
    "# Import Chroma from langchain_community.vectorstores\n",
    "# Chroma is a vector database for storing embeddings of documents.\n",
    "# Useful for semantic search and retrieval in LLM applications.\n",
    "from langchain_community.vectorstores import Chroma  \n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Embedding Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load LLM model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(model_name=\"openai/gpt-oss-120b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Web Document Loader and Vector Store Setup with LangChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embeddings,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Retrieval Grader**\n",
    "\n",
    "This script grades retrieved documents for relevance to a user's question using an LLM and outputs a binary score: **'yes'** or **'no'**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "### MAIN TITLE: Retrieval Grader\n",
    "# This script uses an LLM to assess the relevance of retrieved documents \n",
    "# to a user's question. The goal is to filter out irrelevant retrievals \n",
    "# by assigning a binary score: \"yes\" (relevant) or \"no\" (not relevant).\n",
    "\n",
    "# Import necessary modules\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# -------------------------\n",
    "# Define data model for grading\n",
    "# -------------------------\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"\n",
    "    Binary score for relevance check on retrieved documents.\n",
    "    Fields:\n",
    "    - binary_score: 'yes' if document is relevant, 'no' if not.\n",
    "    \"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# Connect the LLM to the structured output model\n",
    "# -------------------------\n",
    "# structured_llm_grader ensures the LLM output follows the GradeDocuments model\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# -------------------------\n",
    "# Create the grading prompt\n",
    "# -------------------------\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \n",
    "It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n",
    "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \n",
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "# Human template specifies input placeholders for document and question\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Combine prompt and LLM\n",
    "# -------------------------\n",
    "# This creates a \"retrieval_grader\" pipeline\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "# -------------------------\n",
    "# Example usage\n",
    "# -------------------------\n",
    "question = \"agent memory\"            # User question\n",
    "docs = retriever.invoke(question)    # Retrieve documents using some retriever\n",
    "doc_txt = docs[1].page_content      # Select the second document for grading\n",
    "\n",
    "# Invoke the grader and print the result\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In LLM‚Äëpowered agents, **short‚Äëterm memory** is the in‚Äëcontext information the model can use during a single prompt, effectively the model‚Äôs immediate context window. **Long‚Äëterm memory** is an external store (often a vector database) that lets the agent retain and retrieve unlimited knowledge across sessions. Together they let the agent recall past actions and relevant facts while solving tasks.\n"
     ]
    }
   ],
   "source": [
    "### MAIN TITLE: Generate\n",
    "\n",
    "# -------------------------\n",
    "# Import necessary modules\n",
    "# -------------------------\n",
    "from langchain_classic import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# -------------------------\n",
    "# Load a prebuilt RAG (Retrieval-Augmented Generation) prompt\n",
    "# -------------------------\n",
    "# 'rlm/rag-prompt' is a template for generating answers based on context documents\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# -------------------------\n",
    "# Define a post-processing function for documents\n",
    "# -------------------------\n",
    "# Joins the page_content of each document into a single string separated by newlines\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# -------------------------\n",
    "# Build the RAG chain\n",
    "# -------------------------\n",
    "# Pipeline: prompt -> LLM -> output parser\n",
    "# StrOutputParser ensures the final output is a plain string\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# -------------------------\n",
    "# Run the generation\n",
    "# -------------------------\n",
    "# Pass the context documents and the user question to the RAG chain\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "\n",
    "# Print the generated answer\n",
    "print(generation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **Hallucination Grader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "### MAIN TITLE: Hallucination Grader\n",
    "# This script grades an LLM-generated answer for hallucinations. \n",
    "# It outputs a binary score: 'yes' if the answer is grounded in the provided facts, \n",
    "# 'no' if it contains hallucinations or unsupported claims.\n",
    "\n",
    "# -------------------------\n",
    "# Define the data model for grading\n",
    "# -------------------------\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"\n",
    "    Binary score for hallucination presence in the generated answer.\n",
    "    Fields:\n",
    "    - binary_score: 'yes' if the answer is grounded in facts, 'no' if hallucinated.\n",
    "    \"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# Connect the LLM to the structured output model\n",
    "# -------------------------\n",
    "# Ensures the LLM output follows the GradeHallucinations model\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# -------------------------\n",
    "# Create the hallucination grading prompt\n",
    "# -------------------------\n",
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts.\n",
    "Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "\n",
    "# Human template specifies input placeholders for facts (documents) and generation\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Combine the prompt and LLM\n",
    "# -------------------------\n",
    "# This creates a \"hallucination_grader\" pipeline\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "\n",
    "# -------------------------\n",
    "# Run the hallucination grading\n",
    "# -------------------------\n",
    "# Pass the set of documents (facts) and the generated answer\n",
    "result = hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})\n",
    "\n",
    "# Print the grading result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Answer Grader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "### MAIN TITLE: Answer Grader\n",
    "# This script grades an LLM-generated answer to check if it properly addresses the user's question.\n",
    "# It outputs a binary score: 'yes' if the answer resolves the question, 'no' if it does not.\n",
    "\n",
    "# -------------------------\n",
    "# Define the data model for grading\n",
    "# -------------------------\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"\n",
    "    Binary score to assess whether the generated answer addresses the user's question.\n",
    "    Fields:\n",
    "    - binary_score: 'yes' if the answer resolves the question, 'no' otherwise.\n",
    "    \"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# Connect the LLM to the structured output model\n",
    "# -------------------------\n",
    "# Ensures that the LLM output follows the GradeAnswer schema\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# -------------------------\n",
    "# Create the answer grading prompt\n",
    "# -------------------------\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question.\n",
    "Give a binary score 'yes' or 'no'. 'Yes' means that the answer resolves the question.\"\"\"\n",
    "\n",
    "# Human template specifies input placeholders for the question and generated answer\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Combine the prompt and LLM\n",
    "# -------------------------\n",
    "# This creates a \"answer_grader\" pipeline\n",
    "answer_grader = answer_prompt | structured_llm_grader\n",
    "\n",
    "# -------------------------\n",
    "# Run the answer grading\n",
    "# -------------------------\n",
    "# Pass the user question and the generated answer to the grader\n",
    "result = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "\n",
    "# Print the grading result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question Re-writer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Improved question:**  \n",
      "*What is agent memory, and how is it implemented and used in AI or autonomous agents for storing and retrieving information?*\n"
     ]
    }
   ],
   "source": [
    "### **Question Re-writer**\n",
    "# This script rewrites a user question to an optimized version that improves \n",
    "# retrieval performance from a vectorstore. It focuses on understanding the semantic intent \n",
    "# of the original question.\n",
    "\n",
    "# -------------------------\n",
    "# Create the system prompt\n",
    "# -------------------------\n",
    "system = \"\"\"You are a question re-writer that converts an input question to a better version \n",
    "that is optimized for vectorstore retrieval. Look at the input and try to reason \n",
    "about the underlying semantic intent / meaning.\"\"\"\n",
    "\n",
    "# -------------------------\n",
    "# Create the ChatPromptTemplate\n",
    "# -------------------------\n",
    "# The human template specifies the placeholder for the original question\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\nFormulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Build the question rewriter pipeline\n",
    "# -------------------------\n",
    "# Pipeline: prompt -> LLM -> string output\n",
    "# StrOutputParser ensures the output is returned as plain text\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "\n",
    "# -------------------------\n",
    "# Run the question rewriter\n",
    "# -------------------------\n",
    "# Pass the original question to the rewriter and get the improved question\n",
    "improved_question = question_rewriter.invoke({\"question\": question})\n",
    "\n",
    "# Print the improved question\n",
    "print(improved_question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Graph State**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### **This code defines a structured data type to represent the state of a RAG pipeline.**\n",
    "### **It keeps track of the current question, the LLM-generated answer, and the retrieved documents.**\n",
    "\n",
    "### -------------------------\n",
    "### Import necessary modules\n",
    "### -------------------------\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "### -------------------------\n",
    "### Define GraphState as a TypedDict\n",
    "### -------------------------\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: The user question (string)\n",
    "        generation: The answer generated by the LLM (string)\n",
    "        documents: List of retrieved documents (list of strings)\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Nodes and Edges**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN TITLE: Nodes and Edges\n",
    "# This code defines the **nodes** (operations) and **edges** (decisions) of a RAG-style pipeline.\n",
    "# Each node updates the graph state, and edges determine the next node based on conditions.\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# =========================\n",
    "# NODES\n",
    "# =========================\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents for the current question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state, must include 'question'.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated state with 'documents' key containing retrieved documents.\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieve documents from vectorstore or retriever\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate an answer using RAG (retrieval-augmented generation) pipeline.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state, must include 'question' and 'documents'.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated state with 'generation' key containing LLM output.\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Generate answer using RAG chain\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Filter retrieved documents to keep only those relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state, must include 'question' and 'documents'.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated state with 'documents' containing only relevant documents.\n",
    "    \"\"\"\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        # Grade each document for relevance\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Rewrites the question to improve retrieval results.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state, must include 'question'.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated state with 'question' replaced by improved question.\n",
    "    \"\"\"\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write the question using question rewriter\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# EDGES\n",
    "# =========================\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Decide whether to generate an answer or re-write the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state.\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to execute (\"generate\" or \"transform_query\").\n",
    "    \"\"\"\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # If no relevant documents remain, re-write the question\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # If there are relevant documents, proceed to generation\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Check whether the generated answer is grounded in documents and addresses the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state, must include 'question', 'documents', and 'generation'.\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next step (\"useful\", \"not useful\", or \"not supported\").\n",
    "    \"\"\"\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    # Check if generation is supported by documents\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    grade = score.binary_score\n",
    "\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check if generation answers the question\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN TITLE: RAG Workflow StateGraph\n",
    "# This code defines the complete RAG pipeline as a state graph using `langgraph`.\n",
    "# Each node represents an operation (e.g., retrieval, generation, grading),\n",
    "# and edges define the flow and conditional transitions between nodes.\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "# -------------------------\n",
    "# Initialize the workflow graph\n",
    "# -------------------------\n",
    "# 'GraphState' defines the structure of the state that flows through the nodes\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# -------------------------\n",
    "# Define the nodes\n",
    "# -------------------------\n",
    "# Each node is a function that performs a specific operation on the graph state\n",
    "workflow.add_node(\"retrieve\", retrieve)               # Node for document retrieval\n",
    "workflow.add_node(\"grade_documents\", grade_documents) # Node for grading document relevance\n",
    "workflow.add_node(\"generate\", generate)               # Node for RAG generation\n",
    "workflow.add_node(\"transform_query\", transform_query) # Node for question rewriting\n",
    "\n",
    "# -------------------------\n",
    "# Build the graph edges\n",
    "# -------------------------\n",
    "# Define the sequence and conditional flow between nodes\n",
    "\n",
    "# Start the workflow with the 'retrieve' node\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "\n",
    "# After retrieval, grade the documents\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "# Conditional edges based on document grading\n",
    "# decide_to_generate returns \"transform_query\" or \"generate\" depending on document relevance\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Re-run retrieval after transforming the query\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "\n",
    "# Conditional edges based on generation evaluation\n",
    "# grade_generation_v_documents_and_question returns \"useful\", \"not useful\", or \"not supported\"\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",       # Regenerate if answer not grounded\n",
    "        \"useful\": END,                     # End workflow if answer is good\n",
    "        \"not useful\": \"transform_query\",   # Rewrite query if answer doesn't address question\n",
    "    },\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Compile the workflow\n",
    "# -------------------------\n",
    "# Converts the defined nodes and edges into an executable app\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "('Agent memory comes in two main forms.\\u202fShort‚Äëterm memory is the model‚Äôs '\n",
      " 'in‚Äëcontext window: the current prompt and recent dialogue are fed directly '\n",
      " 'to the LLM, letting it ‚Äúremember‚Äù information for the immediate turn.\\u202f'\n",
      " 'Long‚Äëterm memory is an external store (often a vector database) that logs '\n",
      " 'experiences or facts in natural‚Äëlanguage form and can be queried later, '\n",
      " 'giving the agent essentially unlimited recall across sessions.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\"question\": \"Explain how the different types of agent memory work?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "('Chain‚Äëof‚Äëthought (CoT) prompting asks the model to ‚Äúthink step‚Äëby‚Äëstep,‚Äù '\n",
      " 'turning a complex query into a sequence of simpler reasoning steps that are '\n",
      " 'generated in the prompt itself. By explicitly enumerating these intermediate '\n",
      " 'thoughts, the model can use more test‚Äëtime computation to break the problem '\n",
      " 'into manageable sub‚Äëtasks and produce a clearer, more accurate answer. This '\n",
      " 'technique improves performance on difficult tasks by making the model‚Äôs '\n",
      " 'reasoning process visible and controllable.')\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"Explain how chain of thought prompting works?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangGraph-End-to-End-Course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
